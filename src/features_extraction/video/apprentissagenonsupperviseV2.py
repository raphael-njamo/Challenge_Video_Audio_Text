# -*- coding: utf-8 -*-
"""ApprentissageNonSuppervise

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ANlrgJ2lPVdI6bTOkSnkn3y15ttB1lq
"""

# from oauth2client.client import GoogleCredentials
from google.colab import files, drive, auth
import os, os.path

drive.mount("/content/gdrive", force_remount=False)

PATH = "/content/gdrive/My Drive/PROJET TAV/"

if os.path.isfile(f"{PATH}data.zip") and not os.path.isdir("data"):
    print("\nUnziping the data...")
    !unzip -q gdrive/My\ Drive/Projet\ Airbus/data_airbus.zip
    print("Done.")
else:
    print("\nData directory already ready.")

# % matplotlib inline

import time
import random
import cv2
import glob
import keras
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, f1_score, silhouette_score

import pandas as pd
import numpy as np

path_ = os.path.join(PATH,"data")
 glob_ = glob.iglob(f"{path_}/*")
 sequences = [os.path.basename(seq) for seq in glob_]
 folders = list(map(lambda sequence: os.path.join(path_,sequence),sequences))               

def NumberImageBySequence():
  """ Return a data frame with the  number of frames by sequence """
    stats=[]
    for folder in folders :
            images = [os.path.basename(img) for img in glob.iglob(f"{folder}/*.jpg")]
            image_count = len(images)
            stats.append({ "NbrImages": image_count,
                           "CodSeq": "S"+str(int(os.path.basename(folder).split("_")[1])),
                           "Sequence": os.path.basename(folder)})
    df = pd.DataFrame(stats)
    return df

df = NumberImageBySequence()
df

def loadImages(sequences):
    """
    INPUT  sequences : list of sequence name like SEQ_001_VIDEO, SEQ_002_VIDEO,...
    OUTPUT images : list of ndArray of shape (224,224,3) for each image
           labels : Code of each sequence like S1,S2,...
    """
    images_ = []
    labels = []
    PATH = "/content/gdrive/My Drive/PROJET TAV/"
    for sequence in sequences: 
        path_ = os.path.join(PATH,"data",sequence)
        images = glob.iglob(f"{path_}/*.jpg")
        for image in images:                 
            # build file path
            image_path = os.path.join(path_, image)
        
            # Read the image
            img = cv2.imread(image_path)

            # Resize it to 224 x 224 because is the best size of VGG19
            img = cv2.resize(img, (224,224))

            # Convert it from BGR to RGB so we can plot them later (because openCV reads images as BGR)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Now we add it to our array
            codSeq = "S"+str(int(sequence.split("_")[1]))
            images_.append(img)
            labels.append(codSeq)

    return images_, labels

a,b = loadImages(df["Sequence"])
len(a)

df["Sequence"]

# Display Photos
sequences = df["Sequence"]
images, labels = loadImages(sequences)
#labels

def showRandomImages(images, labels, number_of_images_to_show=2):
    """ plot 2 randon images 
  
    INPUT  images : list ndArray of each image
           labels : list of label (like S1,S2,...) for each image
    """

    for code in list(set(labels)):
      
        indicies = [i for i, label in enumerate(labels) if label == code]
        random_indicies = [random.choice(indicies) for i in range(number_of_images_to_show)]
        figure, axis = plt.subplots(1, number_of_images_to_show)

        print("{} random images for codSeq {}".format(number_of_images_to_show, code))

        for image in range(number_of_images_to_show):
            axis[image].imshow(images[random_indicies[image]])
        plt.show()

showRandomImages(images, labels, number_of_images_to_show=2)

def normaliseImages(images, labels):
    """" for normalize each image"""
    
    # Convert to numpy arrays
    images = np.array(images, dtype=np.float32)
    labels = np.array(labels)

    # Normalise the images
    images /= 255
    
    return images, labels

images, labels = normaliseImages(images, labels)
labels



def shuffleData(images, labels):
    """" Shuffle data, only with train"""

    # Just to shuffle our data. so not to have test part.
    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0, random_state=8)  #test_size = 0 because we don't need test part
    
    return X_train, y_train

X_train, y_train = shuffleData(images, labels)
#X_train[0][2]

"""**REDUCTION DE DIMMENSION** 2 méthodes :
 Deep learning (vgg19,vgg16,resnet50)  et   
 ACP
"""

# 1) Deep Learning

# Load the models with ImageNet weights // On enlève les 1ere couches denses

model_vgg16 = keras.applications.vgg16.VGG16(include_top=False, weights="imagenet", input_shape=(224,224,3))

model_vgg19 = keras.applications.vgg19.VGG19(include_top=False, weights="imagenet", input_shape=(224,224,3))

model_resnet50 = keras.applications.resnet50.ResNet50(include_top=False, weights="imagenet", input_shape=(224,224,3))

#model_vgg16.summary()

#appliquer un flatten sur la dim 3D du model cnn pour pourvoir les appliquer sur les algo de clustering

def CNNtransform(model, raw_images):
    """" dimension reduction
    INPUT  model : model of cnn like vgg16,vgg19,resnet50
           raw_images : ndArray nornalize for an image
     """

    # Pass our training data through the network
    predicted = model.predict(raw_images)

    # Flatten the array
    flat = predicted.reshape(raw_images.shape[0], -1)
    
    return flat

Result_vgg16 = CNNtransform(model_vgg16, X_train)
print("VGG16 flattened output has {} features".format(Result_vgg16.shape[1]))

Result_vgg19 = CNNtransform(model_vgg19, X_train)
print("VGG19 flattened output has {} features".format(Result_vgg19.shape[1]))

Result_resnet50 = CNNtransform(model_resnet50, X_train)
print("ResNet50 flattened output has {} features".format(Result_resnet50.shape[1]))

"""on voit que chaque image à 25088 (pour les 2 1er models) features ce qui est considerablement reduit vue qu'on avait 224x224x3=150528 features/pixels au depart
Cette reduction va nous aider pour nos algo de clustering.

KMeans peut travailler avec cette dim reducte mais le GMM n'ont et l'ordi risque de manquer de RAM --> Une PCA
"""

# 2) PCA

def createFitPCA(data, n_components=None): 
    """" create a PCA instance fit it on data and return the instance """
    
    p = PCA(n_components=n_components, random_state=8)
    p.fit(data)
    
    return p

# create a PCA instance for each model
pca_vgg16 = createFitPCA(Result_vgg16)
pca_vgg19 = createFitPCA(Result_vgg19)
pca_resnet50 = createFitPCA(Result_resnet50)

def pcaCumSumPlot(pca):
    """" plot variance cum """
    plt.plot(np.cumsum(pca.explained_variance_ratio_))
    plt.xlabel('nb de composantes')
    plt.ylabel('var expliquée cumulative')
    plt.show()

#pca_vgg16.explained_variance_ratio_

pcaCumSumPlot(pca_vgg16)
pcaCumSumPlot(pca_vgg19)
pcaCumSumPlot(pca_resnet50)

# PCA transformations su les resultat de cnn
ResultPCAvgg16 = pca_vgg16.transform(Result_vgg16)
ResultPCAvgg19 = pca_vgg19.transform(Result_vgg19)
ResultPCAresnet50 = pca_resnet50.transform(Result_resnet50)

len(labels)

#CLUSTERING :: 1) Kmeans

def createTrainKmeans(data, number_of_clusters=len(sequences)):   #number_of_clusters a faire varier dans tout le corpus
    # n_jobs is set to -1 to use all available CPU cores. This makes a big difference on an 8-core CPU
    # especially when the data size gets much bigger. #perfMatters
    
    k = KMeans(n_clusters=number_of_clusters, n_jobs=-1, random_state=8) # n_jobs = -1 to use all available CPU cores

    # Let's do some timings to see how long it takes to train.
    start = time.time()

    # Train it up
    k.fit(data)

    # Stop the timing 
    end = time.time()

    # And see how long that took
    print("Training took {} seconds".format(end-start))
    
    return k

#CLUSTERING :: 2)GMM
def createTrainGMM(data, number_of_clusters=len(sequences)):
    g = GaussianMixture(n_components=number_of_clusters, covariance_type="full", random_state=8)
    
    start=time.time()
    g.fit(data)
    end=time.time()
    
    print("Training took {} seconds".format(end-start))
    
    return g

# Let's pass the data into the algorithm and predict who lies in which cluster. 
# Since we're using the same data that we trained it on, this should give us the training results.

# Here we create and fit a KMeans model with the PCA outputs
print("KMeans (PCA): \n")

print("VGG16")
K_vgg16_pca = createTrainKmeans(ResultPCAvgg16)

print("\nVGG19")
K_vgg19_pca = createTrainKmeans(ResultPCAvgg19)

print("\nResNet50")
K_resnet50_pca = createTrainKmeans(ResultPCAresnet50)

print("-------------------------------------------------","\n")

# Same for Gaussian Model
print("GMM (PCA): \n")

print("VGG16")
G_vgg16_pca = createTrainGMM(ResultPCAvgg16)

print("\nVGG19")
G_vgg19_pca = createTrainGMM(ResultPCAvgg19)

print("\nResNet50")
G_resnet50_pca = createTrainGMM(ResultPCAresnet50)

print("----------------- Sans le PCA ---------------------------","\n")

# Let's also create models for the covnet outputs without PCA for comparison
print("KMeans: \n")

print("VGG16:")
K_vgg16 = createTrainKmeans(Result_vgg16)

print("\nVGG19:")
K_vgg19 = createTrainKmeans(Result_vgg19)

print("\nResNet50:")
K_resnet50 = createTrainKmeans(Result_resnet50)


### Same for Gaussian Model without PCA return MemoryError
#print("GMM : \n")

#print("VGG16")
#G_vgg16 = createTrainGMM(Result_vgg16)

#print("\nVGG19")
#G_vgg19 = createTrainGMM(Result_vgg19)

#print("\nResNet50")
#G_resnet50 = createTrainGMM(Result_resnet50)

# Now we get the custer model predictions


# KMeans with CovNet outputs
k_vgg16_pred = K_vgg16.predict(Result_vgg16)
k_vgg19_pred = K_vgg19.predict(Result_vgg19)
k_resnet50_pred = K_resnet50.predict(Result_resnet50)

# KMeans with PCA outputs
k_vgg16_pred_pca = K_vgg16_pca.predict(ResultPCAvgg16)
k_vgg19_pred_pca = K_vgg19_pca.predict(ResultPCAvgg19)
k_resnet50_pred_pca = K_resnet50_pca.predict(ResultPCAresnet50)

# Gaussian Mixture with PCA outputs
g_vgg16_pred_pca = G_vgg16_pca.predict(ResultPCAvgg16)
g_vgg19_pred_pca = G_vgg19_pca.predict(ResultPCAvgg19)
g_resnet50_pred_pca = G_resnet50_pca.predict(ResultPCAresnet50)

K_vgg19_pca.labels_

silhouette_score(ResultPCAvgg19, K_vgg19_pca.labels_)
#ResultPCAvgg19.shape

k_vgg19_pred_pca

ResultPCAvgg16.shape

"""We now need to count how many of each label are in each cluster, this way we can take a look and if sufficient eperation has happened we can quicly see which cluster is which label. So let's write a function that does that."""

def clusterLabelCount(clusters, labels):
    
    count = {}
    
    # Get unique clusters and labels
    unique_clusters = list(set(clusters))
    unique_labels = list(set(labels))
    
    # Create counter for each cluster/label combination and set it to 0
    for cluster in unique_clusters:
        count[cluster] = {}
        
        for label in unique_labels:
            count[cluster][label] = 0
    
    # Let's count
    for i in range(len(clusters)):
        count[clusters[i]][labels[i]] +=1
    
    cluster_df = pd.DataFrame(count)
    
    return cluster_df

# Cluster counting for VGG16 Means
vgg16_cluster_count = clusterLabelCount(k_vgg16_pred, y_train)
vgg16_cluster_count_pca = clusterLabelCount(k_vgg16_pred_pca, y_train)

# VGG19 KMeans
vgg19_cluster_count = clusterLabelCount(k_vgg19_pred, y_train)
vgg19_cluster_count_pca = clusterLabelCount(k_vgg19_pred_pca, y_train)

# ResNet50 KMeans
resnet_cluster_count = clusterLabelCount(k_resnet50_pred, y_train)
resnet_cluster_count_pca = clusterLabelCount(k_resnet50_pred_pca, y_train)

# GMM
g_vgg16_cluster_count_pca = clusterLabelCount(g_vgg16_pred_pca, y_train)
g_vgg19_cluster_count_pca = clusterLabelCount(g_vgg19_pred_pca, y_train)
g_resnet50_cluster_count_pca = clusterLabelCount(g_resnet50_pred_pca, y_train)

print("KMeans VGG16: ")
vgg16_cluster_count

K_vgg16_pca.labels_

y_train

print("KMeans VGG16 (PCA): ")
vgg16_cluster_count_pca

vgg16_cluster_code_pca = ["S5", "S5", "S1", "S1"]

print("KMeans VGG16 : ")
vgg16_cluster_count

print("KMeans VGG19 (PCA): ")
vgg19_cluster_count_pca

vgg19_cluster_code_pca = ["S4","S5","S5","S1"]

print("KMeans VGG19: ")
vgg19_cluster_count

g_vgg19_cluster_count_pca

#assignation manuelle de labels aux clusters
vgg16_cluster_code_pca = ["S5", "S5", "S1", "S1"]
vgg16_cluster_code = ["S5", "S5", "S1", "S1"]

vgg19_cluster_code_pca = ["S4","S5","S5","S1"]
vgg19_cluster_code = ["S4","S5","S5","S1"]

g_vgg19_cluster_code_pca = ["S5", "S5", "S1", "S1"]

#remplace les cluster predicts par leur labels
vgg16_pred_codes = [vgg16_cluster_code[x] for x in k_vgg16_pred]
vgg16_pred_codes_pca = [vgg16_cluster_code_pca[x] for x in k_vgg16_pred_pca]
vgg19_pred_codes = [vgg19_cluster_code[x] for x in k_vgg19_pred]
vgg19_pred_codes_pca = [vgg19_cluster_code_pca[x] for x in k_vgg19_pred_pca]
g_vgg19_pred_codes_pca = [g_vgg19_cluster_code_pca[x] for x in g_vgg19_pred_pca]

np.array(vgg16_pred_codes)

y_train

"""Avec nos deux tableau, predict et train on peut calculer le score de prediction"""

def print_scores(true, pred):
    acc = accuracy_score(true, pred)
    f1 = f1_score(true, pred, average="macro")
    return "\n\tF1 Score: {0:0.8f}%   |   Accuracy: {0:0.8f}%".format(f1*100,acc*100)

print("KMeans VGG16:", print_scores(y_train, vgg16_pred_codes))
print("KMeans VGG16 (PCA)", print_scores(y_train, vgg16_pred_codes_pca))

print("\nKMeans VGG19: ", print_scores(y_train, vgg19_pred_codes))
print("KMeans VGG19 (PCA): ", print_scores(y_train, vgg19_pred_codes_pca))
print("GMM VGG19 (PCA)", print_scores(y_train, g_vgg19_pred_codes_pca))

def all_covnet_transform(data):
    vgg16 = CNNtransform(model_vgg16, data)
    vgg19 = CNNtransform(model_vgg19, data)
    resnet50 = CNNtransform(model_resnet50, data)
    
    return vgg16, vgg19, resnet50


def image_load_to_cluster_count(codes):
    # Load images
    images, labels = loadImages(codes)   #codes == sequences name
    print(len(images), len(labels))
    showRandomImages(images, labels)
    
    # Normalise images
    images, labels = normaliseImages(images, labels)
    
    # Split data
    data, labels = shuffleData(images, labels)
    
    # Get covnet outputs
    vgg16_output, vgg19_output, resnet50_output = all_covnet_transform(data)
    
    # Get PCA transformations
    vgg16_output_pca = createFitPCA(vgg16_output).transform(vgg16_output)
    vgg19_output_pca = createFitPCA(vgg19_output).transform(vgg19_output)
    resnet50_output_pca = createFitPCA(resnet50_output).transform(resnet50_output)
    
        # Cluster
    clusters = len(codes)
    
    K_vgg16_pred = createTrainKmeans(vgg16_output, clusters).predict(vgg16_output)
    K_vgg19_pred =  createTrainKmeans(vgg19_output, clusters).predict(vgg19_output)
    K_resnet50_pred = createTrainKmeans(resnet50_output, clusters).predict(resnet50_output)
    K_vgg16_pred_pca = createTrainKmeans(vgg16_output_pca, clusters).predict(vgg16_output_pca)
    K_vgg19_pred_pca = createTrainKmeans(vgg19_output_pca, clusters).predict(vgg19_output_pca)
    K_resnet50_pred_pca = createTrainKmeans(resnet50_output_pca, clusters).predict(resnet50_output_pca)
    G_vgg16_pred_pca = createTrainGMM(vgg16_output_pca, clusters).predict(vgg16_output_pca)
    G_vgg19_pred_pca = createTrainGMM(vgg19_output_pca, clusters).predict(vgg19_output_pca)
    G_resnet50_pred_pca = createTrainGMM(resnet50_output_pca, clusters).predict(resnet50_output_pca)
    
    # Count
    vgg16_cluster_count = clusterLabelCount(K_vgg16_pred, labels)
    vgg16_cluster_count_pca = clusterLabelCount(K_vgg16_pred_pca, labels)

    # VGG19 KMeans
    vgg19_cluster_count = clusterLabelCount(K_vgg19_pred, labels)
    vgg19_cluster_count_pca = clusterLabelCount(K_vgg19_pred_pca, labels)

    # ResNet50 KMeans
    resnet_cluster_count = clusterLabelCount(K_resnet50_pred, labels)
    resnet_cluster_count_pca = clusterLabelCount(K_resnet50_pred_pca, labels)

    # GMM
    g_vgg16_cluster_count_pca = clusterLabelCount(G_vgg16_pred_pca, labels)
    g_vgg19_cluster_count_pca = clusterLabelCount(G_vgg19_pred_pca, labels)
    g_resnet50_cluster_count_pca = clusterLabelCount(G_resnet50_pred_pca, labels)
    
    
    print("KMeans VGG16: ")
    print(vgg16_cluster_count)
    print("\nKMeans VGG16 (PCA): ")
    print(vgg16_cluster_count_pca)
    print("\nGMM VGG16: ")
    print(g_vgg16_cluster_count_pca)
    print("\nKMeans VGG19: ")
    print(vgg19_cluster_count)
    print("\nKMeans VGG19 (PCA): ")
    print(vgg19_cluster_count_pca)
    print("GMM VGG19 (PCA): ")
    print(g_vgg19_cluster_count_pca)
    print("KMeans Resnet50: ")
    print(resnet_cluster_count)
    print("Kmeans Resnet50 (PCA): ")
    print(resnet_cluster_count_pca)
    print("GMM Resnet50 (PCA): ")
    print(g_resnet50_cluster_count_pca)
    
    return  K_vgg16_pred, K_vgg16_pred_pca, K_vgg19_pred, K_vgg19_pred_pca, G_vgg19_pred_pca, images, labels

seq = ["SEQ_001_VIDEO"]
image_load_to_cluster_count(seq)

df["Sequence"]